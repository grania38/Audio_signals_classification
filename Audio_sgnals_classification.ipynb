{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ya8Gi6lyV1p"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting dataset to a directory called ESC-50"
      ],
      "metadata": {
        "id": "oJzrF4wpya3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "if not os.path.exists(\"ESC-50\"):\n",
        "    os.makedirs(\"ESC-50\")\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/ESC-50-master.zip', \"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"ESC-50\")\n",
        "\n",
        "print(\"Dataset extracted to directory:\", \"ESC-50\")"
      ],
      "metadata": {
        "id": "SK_dtz7fyXP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wav_files = [file for file in os.listdir(\"/content/ESC-50/ESC-50-master/audio\") if file.endswith(\".wav\")]"
      ],
      "metadata": {
        "id": "2AoEo2e3yXOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining some libraries and two dictionaries one considered as an encoder of the dataset classes and the other as a decoder"
      ],
      "metadata": {
        "id": "vQo2vEHhyh2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import random\n",
        "import soundfile as sf\n",
        "import librosa.display\n",
        "#this is a dictionary to encode the categories into targets\n",
        "encoder = {'dog': 0, 'chirping_birds': 14, 'vacuum_cleaner': 36, 'thunderstorm': 19, 'door_wood_knock': 30, 'can_opening': 34, 'crow': 9, 'clapping': 22, 'fireworks': 48, 'chainsaw': 41, 'airplane': 47, 'mouse_click': 31, 'pouring_water': 17, 'train': 45, 'sheep': 8, 'water_drops': 15, 'church_bells': 46, 'clock_alarm': 37, 'keyboard_typing': 32, 'wind': 16, 'footsteps': 25, 'frog': 4, 'cow': 3, 'brushing_teeth': 27, 'car_horn': 43, 'crackling_fire': 12, 'helicopter': 40, 'drinking_sipping': 29, 'rain': 10, 'insects': 7, 'laughing': 26, 'hen': 6, 'engine': 44, 'breathing': 23, 'crying_baby': 20, 'hand_saw': 49, 'coughing': 24, 'glass_breaking': 39, 'snoring': 28, 'toilet_flush': 18, 'pig': 2, 'washing_machine': 35, 'clock_tick': 38, 'sneezing': 21, 'rooster': 1, 'sea_waves': 11, 'siren': 42, 'cat': 5, 'door_wood_creaks': 33, 'crickets': 13}\n",
        "#this is a dictionary to decode the categories into targets\n",
        "decoder = {0: 'dog', 14: 'chirping_birds', 36: 'vacuum_cleaner', 19: 'thunderstorm', 30: 'door_wood_knock',34: 'can_opening', 9: 'crow', 22: 'clapping', 48: 'fireworks', 41: 'chainsaw', 47: 'airplane', 31: 'mouse_click', 17: 'pouring_water', 45: 'train', 8: 'sheep', 15: 'water_drops', 46: 'church_bells', 37: 'clock_alarm', 32: 'keyboard_typing', 16: 'wind', 25: 'footsteps', 4: 'frog', 3: 'cow', 27: 'brushing_teeth', 43: 'car_horn', 12: 'crackling_fire', 40: 'helicopter', 29: 'drinking_sipping', 10: 'rain', 7: 'insects', 26: 'laughing', 6: 'hen', 44: 'engine', 23: 'breathing', 20: 'crying_baby', 49: 'hand_saw', 24: 'coughing', 39: 'glass_breaking', 28: 'snoring', 18: 'toilet_flush', 2: 'pig', 35: 'washing_machine', 38: 'clock_tick', 21: 'sneezing', 1: 'rooster', 11: 'sea_waves', 42: 'siren', 5: 'cat', 33: 'door_wood_creaks', 13: 'crickets'}"
      ],
      "metadata": {
        "id": "ZOnaxbIMyXKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A. Exploring the Audio Data\n",
        "Let's first explore a random audio sample of dog bark 1-100032-A-0.wav by printing the values of :\n",
        "\n",
        "Sample rate: it is the number of samples per second.<br>\n",
        "Amplitude: it's the measure of how high/ low the wave extends from the x axis."
      ],
      "metadata": {
        "id": "ARpCNXX4yn-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing one file and calculate its duration, amplitude and shape\n",
        "import numpy as np\n",
        "y, sr = librosa.load(\"/content/ESC-50/ESC-50-master/audio/1-100032-A-0.wav\")\n",
        "print('amplitude y:', y)\n",
        "print('y shape:', np.shape(y))\n",
        "print('the duration of the audio:', np.shape(y)[0]/sr, 's')"
      ],
      "metadata": {
        "id": "nP2S_GcmyXIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#listening to the audio file\n",
        "import IPython.display as ipd\n",
        "ipd.Audio(\"/content/ESC-50/ESC-50-master/audio/1-100032-A-0.wav\")"
      ],
      "metadata": {
        "id": "QxTnRTHFyXFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B. Distribution of categories"
      ],
      "metadata": {
        "id": "c5cckklqyxNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The data is described in the .csv file called esc50. Using this file, we will be able to discover our dataset.\n",
        "import pandas as pd\n",
        "df = pd.read_csv('/content/ESC-50/ESC-50-master/meta/esc50.csv')"
      ],
      "metadata": {
        "id": "b294wHV1yXDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# displaying the first 5 rows of the dataset.\n",
        "df.head()"
      ],
      "metadata": {
        "id": "YrPZGbRZyW_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "category_group=df['category'].value_counts() #we are counting how many rows per class\n",
        "colors = sns.color_palette(\"husl\", len(category_group))\n",
        "plot = category_group.plot(kind='bar', title=\"The number of audios per class\", figsize=(20,10), color=colors)\n",
        "plot.set_xlabel(\"Class\")\n",
        "plot.set_ylabel(\"Number of audios\");"
      ],
      "metadata": {
        "id": "rYnz-bbPyW9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing audio signals"
      ],
      "metadata": {
        "id": "hbYBuKccy6wy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ESC-50/ESC-50-master/audio"
      ],
      "metadata": {
        "id": "7S7wder6yW6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_files = random.choices(wav_files, k = 10)\n",
        "plot_audios = [librosa.load(plot_files[i]) for i in range(10)]"
      ],
      "metadata": {
        "id": "VfgMN_MWy6au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the shape of sound waves of some categories such as: sound of church bells, water drops etc...\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(14,10))\n",
        "for i in range(1,7):\n",
        "    plt.subplot(2,3,i)\n",
        "    librosa.display.waveshow(plot_audios[i][0])\n",
        "    try:\n",
        "        plt.title(\"Sound of \" + decoder[int(plot_files[i][-6:-4])] )\n",
        "    except:\n",
        "        plt.title(\"Sound of \" + decoder[int(plot_files[i][-5:-4])] )"
      ],
      "metadata": {
        "id": "lwp17qghy6XT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fourier transform"
      ],
      "metadata": {
        "id": "fgRrak2XzCte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here I am plotting the fourrier transform of these sounds.\n",
        "plt.figure(figsize=(14,10))\n",
        "for i in range(1,7):\n",
        "    plt.subplot(2,3,i)\n",
        "    X = np.abs(librosa.stft(plot_audios[i][0], n_fft =2048, hop_length =512))\n",
        "    plt.plot(X)\n",
        "    plt.xlabel(\"freq\")\n",
        "    plt.ylabel(\"Amplitude\");\n",
        "    try:\n",
        "        plt.title(\"Fourier Transform of \" + decoder[int(plot_files[i][-6:-4])] )\n",
        "    except:\n",
        "        plt.title(\"Fourier Transform of \" + decoder[int(plot_files[i][-5:-4])] )"
      ],
      "metadata": {
        "id": "MYVPJbcJy6Vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## III. Data Preparation <br>\n",
        "A. Data Augmentation <br>\n",
        "Time stretching : This technique changes the duration of the audio signal by speeding it up or slowing it down. This can be useful for simulating variations in the tempo of the audio."
      ],
      "metadata": {
        "id": "4-WZmnFXzSgu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Noise : This technique adds noise to the audio signal to simulate different noise conditions."
      ],
      "metadata": {
        "id": "McT7Z0dgzW-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_noise(path):\n",
        "    noise = np.random.normal(0, 0.1, len(path))\n",
        "    audio_noisy = path + noise\n",
        "    return audio_noisy"
      ],
      "metadata": {
        "id": "vqw-sz6jy6SG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time shifting: This technique changes the position of the audio signal in time by shifting it forwards or backwards."
      ],
      "metadata": {
        "id": "1l44e7MzzZQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random_shift(path):\n",
        "    timeshift_fac = 0.2 *2*(np.random.uniform()-0.5)  # up to 20% of length\n",
        "    start = int(path.shape[0] * timeshift_fac)\n",
        "    if (start > 0):\n",
        "        data = np.pad(path,(start,0),mode='constant')[0:path.shape[0]]\n",
        "    else:\n",
        "        data = np.pad(path,(0,-start),mode='constant')[0:path.shape[0]]\n",
        "    return data"
      ],
      "metadata": {
        "id": "z8XlMsUky6Qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Volume scaling: This technique changes the volume of the audio signal by scaling it up or down. This can be useful for simulating variations in the loudness of the audio."
      ],
      "metadata": {
        "id": "x5AHgx3Tzh_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def volume_scaling(path):\n",
        "    sr  = 16000\n",
        "    dyn_change = np.random.uniform(low=1.5,high=2.5)\n",
        "    data = path * dyn_change\n",
        "    return data"
      ],
      "metadata": {
        "id": "hLutujE_ziuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def aug_audio(file, aug):\n",
        "    directory = 'ESC-50-augmented-data/'\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "    aug = np.array(aug,dtype='float32').reshape(-1,1)\n",
        "    sf.write(directory+'/'+ file, aug, 16000, 'PCM_24')"
      ],
      "metadata": {
        "id": "A4wkcMYuzisx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augmentations(path_audio):\n",
        "    path_ = np.random.choice(os.listdir(path_audio), size = (2000,), replace=False)\n",
        "    for k,files in zip(range(len(path_)), path_):\n",
        "        if files[0] != \"5\":\n",
        "            data_, fs = librosa.load(os.path.join(path_audio, files), sr=16000)\n",
        "            noise =add_noise(data_)\n",
        "            ran_shift= random_shift(data_)\n",
        "            volume_scale= volume_scaling(data_)\n",
        "            l= [noise,ran_shift,volume_scale]\n",
        "            for m in range(len(l)):\n",
        "                filename = (files[0:2]+'generated'+'-'+str(m)+'-'+str(k)+'-'+files[2:])\n",
        "                aug_audio(filename,l[m])"
      ],
      "metadata": {
        "id": "su9bbYWhy6NA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Performing data augmentation\n",
        "path=\"/content/ESC-50/ESC-50-master/audio\"\n",
        "augmentations(path)"
      ],
      "metadata": {
        "id": "WWj51KJvy6Le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IV- Using approach of neural networks"
      ],
      "metadata": {
        "id": "2feqcGYozwEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataGenerator(Dataset):\n",
        "    def __init__(self, path, transform = None, kind='train'):\n",
        "\n",
        "        if kind=='train':\n",
        "            files = Path(path).glob('[1-4]-*')\n",
        "            self.item = [(str(file), file.name.split('-')[-1].replace('.wav', '')) for file in files]\n",
        "        if kind=='test':\n",
        "            files = Path(path).glob('5-*')\n",
        "            self.item= [(str(file), file.name.split('-')[-1].replace('.wav', '')) for file in files]\n",
        "        self.len = len(self.item)\n",
        "        print(self.len)\n",
        "    def __getitem__(self, index):\n",
        "        filename, label = self.item[index]\n",
        "        data_tensor, rate = torchaudio.load(filename)\n",
        "        tmp = data_tensor[0,0:80000]\n",
        "        return (tmp, int(label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len"
      ],
      "metadata": {
        "id": "1VuQeIiozxkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "batch= 64\n",
        "path_audio= '/content/ESC-50/ESC-50-master/audio'\n",
        "train_data = DataGenerator(path_audio, kind='train')\n",
        "test_data = DataGenerator(path_audio, kind='test')\n",
        "#Applying the data loader on the training and testing data\n",
        "train_loader = DataLoader(train_data, batch_size=batch, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch, shuffle=True)"
      ],
      "metadata": {
        "id": "DLLhPKHYzxgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the cnn class\n",
        "class Net(nn.Module):\n",
        "    #Constructor\n",
        "    def __init__(self, num_classes=50):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(1,8), stride=(1,1), padding=\"same\")\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(1,8), stride=(1,1), padding=\"same\")\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "\n",
        "        self.pool_1 = nn.MaxPool2d(kernel_size=(1,128), stride = (1,128), padding=0)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3,3), stride=(1,1), padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3,3), stride=(1,1), padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.pool_2 = nn.MaxPool2d(kernel_size=4, padding=0)\n",
        "\n",
        "        self.conv5 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3,3), stride=(2,2), padding=2)\n",
        "        self.bn5 = nn.BatchNorm2d(64)\n",
        "        self.conv6 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3,3), stride=(2,2), padding=1)\n",
        "        self.bn6 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.pool_3 = nn.MaxPool2d(kernel_size=2, padding=0)\n",
        "\n",
        "        self.conv7 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3,3), stride=(2,2), padding=1)\n",
        "        self.bn7 = nn.BatchNorm2d(128)\n",
        "        self.conv8 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3,3), stride=(2,2), padding=1)\n",
        "        self.bn8 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.pool_4 = nn.MaxPool2d(kernel_size=(1,2), padding=0)\n",
        "\n",
        "        self.dense = nn.Linear(in_features= 256, out_features=num_classes)\n",
        "        # defining the dropout that is used mainly to avoid overfitting\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, a):\n",
        "        a= a.unsqueeze(1).view(-1, 1, 1, 80000)\n",
        "        a= F.relu(self.bn1(self.conv1(a)))\n",
        "        a= self.dropout(a)\n",
        "        a= F.relu(self.bn2(self.conv2(a)))\n",
        "        a= self.pool_1(a)\n",
        "        a= a.view((-1,1,16, 625))\n",
        "        a= F.relu(self.bn3(self.conv3(a)))\n",
        "        a= self.dropout(a)\n",
        "        a= F.relu(self.bn4(self.conv4(a)))\n",
        "        a= self.pool_2(a)\n",
        "        a= F.relu(self.bn5(self.conv5(a)))\n",
        "        a= self.dropout(a)\n",
        "        a= F.relu(self.bn6(self.conv6(a)))\n",
        "        a= self.pool_3(a)\n",
        "        a= F.relu(self.bn7(self.conv7(a)))\n",
        "        a= self.dropout(a)\n",
        "        a= F.relu(self.bn8(self.conv8(a)))\n",
        "        a= self.pool_4(a)\n",
        "        a= a.view(a.size(0),-1)\n",
        "        a= self.dense(a)\n",
        "        a= self.dropout(a)\n",
        "        return a"
      ],
      "metadata": {
        "id": "0OEL40VhzxfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using cuda to make the training phase faster\n",
        "device = \"cpu\"\n",
        "if (torch.cuda.is_available()):\n",
        "    device = \"cuda\""
      ],
      "metadata": {
        "id": "u1xYyirozxZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net(num_classes=50).to(device)\n",
        "input_data = torch.randn(64, 1, 80000)\n",
        "model_graph = draw_graph(model,input_data, roll=True)\n",
        "model_graph.visual_graph"
      ],
      "metadata": {
        "id": "0b9m2W0UzxXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    loss_training =0\n",
        "    print(\"------------------------------- Epoch:\", epoch,\"-------------------------------\")\n",
        "    # Process the images in batches\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # Use the CPU or GPU as appropriate\n",
        "        # Recall that GPU is optimized for the operations we are dealing with\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        # Reset the optimizer\n",
        "        optimizer.zero_grad()\n",
        "        # Push the data forward through the model layers\n",
        "        output = model(data.to(device))\n",
        "        loss_criteria=nn.CrossEntropyLoss()\n",
        "        loss = loss_criteria(output, target)\n",
        "        # Keep a running total\n",
        "        loss_training += loss.item()\n",
        "        # Backpropagate\n",
        "        loss.backward(retain_graph=True)\n",
        "        optimizer.step()\n",
        "    # return average loss for the epoch\n",
        "    avg_loss =loss_training / (batch_idx+1)\n",
        "    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "vvId_Dm9zxUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    loss_testing = 0\n",
        "    true= 0\n",
        "    with torch.no_grad():\n",
        "        batch_count = 0\n",
        "        for data, target in test_loader:\n",
        "            batch_count += 1\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            # adding the predicted classes for the actual batch\n",
        "            output = model(data)\n",
        "            # Calculate the loss for the actual batch\n",
        "            loss_criteria=nn.CrossEntropyLoss()\n",
        "            loss_testing += loss_criteria(output, target).item()\n",
        "            # Calculate the accuracy for this batch\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            true += torch.sum(target==predicted).item()\n",
        "    # Calculate the average loss and total accuracy for this epoch\n",
        "    avg_loss =loss_testing / batch_count\n",
        "    print('Validation set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(avg_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))\n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "6QTpaB-D0Bdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_model(model):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
        "    loss_criteria = nn.CrossEntropyLoss()\n",
        "    epoch_list = []\n",
        "    train_loss_list = []\n",
        "    valid_loss_list= []\n",
        "    epochs = 20\n",
        "    print('Training on', device)\n",
        "    for epoch in tqdm(range(1, epochs + 1)):\n",
        "        train_loss = train(model, device, train_loader, optimizer, epoch)\n",
        "        test_loss = test(model, device, test_loader)\n",
        "        epoch_list.append(epoch)\n",
        "        train_loss_list.append(train_loss)\n",
        "        valid_loss_list.append(test_loss)"
      ],
      "metadata": {
        "id": "6Zzuk8w50BcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio\n",
        "training_model(model)"
      ],
      "metadata": {
        "id": "TlaYRs4C0BW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ne-0ahbkzxSi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}